{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNiScRT74IKHFOCPFSGl8EJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghadiiz/movie-sentiment-analyzer-nlp/blob/main/Movie_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rodl3p0twt-q"
      },
      "outputs": [],
      "source": [
        "def setup_nlp_environment():\n",
        "    \"\"\"Imports common NLP libraries and downloads NLTK data packages.\"\"\"\n",
        "    print(\"Importing necessary libraries...\")\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        import nltk\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.naive_bayes import MultinomialNB\n",
        "        from sklearn.metrics import classification_report, accuracy_score\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            from tensorflow import keras\n",
        "        except ImportError:\n",
        "            print(\"TensorFlow/Keras not found. Skipping import.\")\n",
        "\n",
        "        print(\"Libraries imported successfully.\")\n",
        "\n",
        "        print(\"Downloading NLTK data packages...\")\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        nltk.download('omw-1.4', quiet=True) # Open Multilingual WordNet, often needed with wordnet\n",
        "        nltk.download('punkt_tab', quiet=True) # Added to resolve LookupError\n",
        "        print(\"NLTK data packages downloaded successfully.\")\n",
        "\n",
        "        # Optional: Set up plotting style\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"Error importing a library: {e}. Please ensure all libraries are installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Call the function to set up the environment\n",
        "setup_nlp_environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# 1. Load the IMDB dataset\n",
        "# num_words parameter keeps the most frequent words\n",
        "max_features = 10000  # consider only top 10k words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# 2. Get the word index and create a reverse word index\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# The indices are offset by 3 because 0, 1, and 2 are reserved for \"padding,\" \"start of sequence,\" and \"unknown.\"\n",
        "# Define a decoding function\n",
        "def decode_review(text_sequence):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text_sequence])\n",
        "\n",
        "# 3. Convert integer sequences back to text reviews\n",
        "train_reviews_text = [decode_review(seq) for seq in x_train]\n",
        "test_reviews_text = [decode_review(seq) for seq in x_test]\n",
        "\n",
        "# 4. Create pandas DataFrames\n",
        "# Combine train and test data\n",
        "reviews = train_reviews_text + test_reviews_text\n",
        "sentiments = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "imdb_df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
        "\n",
        "# Map sentiment labels to 'positive' and 'negative' for clarity if desired\n",
        "# imdb_df['sentiment'] = imdb_df['sentiment'].map({0: 'negative', 1: 'positive'})\n",
        "\n",
        "# 5. Display the first few rows, DataFrame shape, and data types\n",
        "print(\"\\n--- IMDB Dataset DataFrame ---\")\n",
        "print(\"First 5 rows:\")\n",
        "print(imdb_df.head())\n",
        "\n",
        "print(f\"\\nDataFrame shape: {imdb_df.shape}\")\n",
        "\n",
        "print(\"\\nDataFrame info:\")\n",
        "imdb_df.info()"
      ],
      "metadata": {
        "id": "rpl0sxSR1sqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n--- Exploratory Data Analysis on IMDB Dataset ---\")\n",
        "\n",
        "# 1) Bar chart showing count of positive vs negative reviews\n",
        "print(\"\\n1. Sentiment Distribution:\")\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(x='sentiment', data=imdb_df)\n",
        "plt.title('Distribution of Sentiments (0: Negative, 1: Positive)')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'])\n",
        "plt.show()\n",
        "\n",
        "# 2) Histogram showing distribution of review lengths in words\n",
        "print(\"\\n2. Review Length Distribution:\")\n",
        "# Calculate review lengths\n",
        "imdb_df['review_length'] = imdb_df['review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(imdb_df['review_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Review Lengths (in words)')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 3) Check for missing values\n",
        "print(\"\\n3. Missing Values Check:\")\n",
        "print(imdb_df.isnull().sum())\n",
        "\n",
        "# 4) Display 5 random sample reviews with their sentiments\n",
        "print(\"\\n4. Five Random Sample Reviews:\")\n",
        "for index, row in imdb_df.sample(5).iterrows():\n",
        "    print(f\"\\nSentiment: {'Positive' if row['sentiment'] == 1 else 'Negative'}\")\n",
        "    print(f\"Review: {row['review']}\")\n",
        "\n",
        "# Drop the temporary 'review_length' column if no longer needed\n",
        "imdb_df = imdb_df.drop(columns=['review_length'])"
      ],
      "metadata": {
        "id": "P0ii4zGw2PkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# from bs4 import BeautifulSoup # Uncomment if you prefer BeautifulSoup for HTML removal and install it\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Performs text preprocessing steps:\n",
        "    1. Converts to lowercase.\n",
        "    2. Removes HTML tags.\n",
        "    3. Removes special characters and punctuation (keeping only letters and spaces).\n",
        "    4. Removes extra whitespaces.\n",
        "    \"\"\"\n",
        "    # 1. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove HTML tags (using regex for simplicity as BeautifulSoup might require installation)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # If using BeautifulSoup: text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    # 3. Remove special characters and punctuation (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # 4. Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to create a new 'cleaned_review' column\n",
        "print(\"Applying text cleaning to 'review' column...\")\n",
        "imdb_df['cleaned_review'] = imdb_df['review'].apply(clean_text)\n",
        "print(\"Cleaning complete. 'cleaned_review' column created.\")\n",
        "\n",
        "# Display before and after examples of 3 random reviews\n",
        "print(\"\\n--- Before and After Cleaning Examples (3 Reviews) ---\")\n",
        "sample_reviews = imdb_df.sample(3)\n",
        "\n",
        "for index, row in sample_reviews.iterrows():\n",
        "    print(f\"\\nOriginal Review:\\n{row['review']}\")\n",
        "    print(f\"Cleaned Review:\\n{row['cleaned_review']}\")\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "7wSuHH7y3r1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Re-import for explicit use if not already in scope, though it is.\n",
        "\n",
        "# Initialize NLTK components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text, removes stopwords, and applies lemmatization.\n",
        "    \"\"\"\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            processed_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "print(\"Applying tokenization, stopword removal, and lemmatization to 'cleaned_review' column...\")\n",
        "imdb_df['processed_review'] = imdb_df['cleaned_review'].apply(process_text)\n",
        "print(\"Processing complete. 'processed_review' column created.\")\n",
        "\n",
        "# Display before and after examples of 3 random reviews\n",
        "print(\"\\n--- Before and After Processing Examples (3 Reviews) ---\")\n",
        "sample_reviews_processed = imdb_df.sample(3)\n",
        "\n",
        "for index, row in sample_reviews_processed.iterrows():\n",
        "    print(f\"\\nOriginal Review (Cleaned):\\n{row['cleaned_review']}\")\n",
        "    print(f\"Processed Review:\\n{row['processed_review']}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "# Display 20 most common words after processing\n",
        "print(\"\\n--- 20 Most Common Words After Processing ---\")\n",
        "all_words = ' '.join(imdb_df['processed_review']).split()\n",
        "word_counts = Counter(all_words)\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Count', y='Word', data=common_words_df)\n",
        "plt.title('20 Most Common Words in Processed Reviews')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Word')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oe1aX2Br4BA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n--- Data Splitting and TF-IDF Vectorization ---\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = imdb_df['processed_review']\n",
        "y = imdb_df['sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets (80/20 split) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Dataset split into training and testing sets.\")\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "# max_features is set to 5000 as requested, considering the top 5000 most frequent words\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit the vectorizer on the training data only\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Text data vectorized using TF-IDF.\")\n",
        "\n",
        "# Display shapes of the resulting datasets\n",
        "print(\"\\nShapes of the datasets:\")\n",
        "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
        "print(f\"X_test_tfidf shape: {X_test_tfidf.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Display vocabulary size\n",
        "vocabulary_size = len(tfidf_vectorizer.get_feature_names_out())\n",
        "print(f\"\\nVocabulary size (number of features): {vocabulary_size}\")\n",
        "\n",
        "# Display a sample of the TF-IDF matrix\n",
        "# Since TF-IDF matrix is sparse, we convert a small part to dense for display\n",
        "print(\"\\nSample of TF-IDF matrix (first 5 rows and first 10 columns):\")\n",
        "print(X_train_tfidf[:5, :10].toarray())\n",
        "\n",
        "print(\"\\nData preparation complete.\")"
      ],
      "metadata": {
        "id": "ECgBbaLI5Axp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Multinomial Naive Bayes Model Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "mnb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Multinomial Naive Bayes classifier...\")\n",
        "mnb_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = mnb_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = mnb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "print(\"\\nModel evaluation complete.\")"
      ],
      "metadata": {
        "id": "_NIAdAP66NUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Logistic Regression Model Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "# max_iter is increased to ensure convergence for the given dataset\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Logistic Regression classifier...\")\n",
        "lr_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred_lr = lr_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred_lr = lr_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy_lr = accuracy_score(y_train, y_train_pred_lr)\n",
        "print(f\"\\nTraining Accuracy (Logistic Regression): {train_accuracy_lr:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
        "print(f\"Test Accuracy (Logistic Regression): {test_accuracy_lr:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set (Logistic Regression):\")\n",
        "print(classification_report(y_test, y_test_pred_lr))\n",
        "\n",
        "print(\"\\nLogistic Regression Model evaluation complete.\")"
      ],
      "metadata": {
        "id": "mV6kCBpm6jsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Random Forest Classifier Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "# n_estimators is the number of trees in the forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Random Forest classifier... (This may take a few minutes)\")\n",
        "rf_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred_rf = rf_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "print(f\"\\nTraining Accuracy (Random Forest): {train_accuracy_rf:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
        "print(f\"Test Accuracy (Random Forest): {test_accuracy_rf:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set (Random Forest):\")\n",
        "print(classification_report(y_test, y_test_pred_rf))\n",
        "\n",
        "print(\"\\nRandom Forest Classifier evaluation complete.\")"
      ],
      "metadata": {
        "id": "drqkME4077Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for LSTM Training\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"--- Preparing Data for LSTM Model ---\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "max_features = 5000  # Maximum number of words to keep\n",
        "maxlen = 200  # Maximum length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "\n",
        "# Fit on processed reviews\n",
        "tokenizer.fit_on_texts(imdb_df['processed_review'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(imdb_df['processed_review'].iloc[:40000])\n",
        "X_test_sequences = tokenizer.texts_to_sequences(imdb_df['processed_review'].iloc[40000:])\n",
        "\n",
        "# Pad sequences to same length\n",
        "X_train_lstm = pad_sequences(X_train_sequences, maxlen=maxlen)\n",
        "X_test_lstm = pad_sequences(X_test_sequences, maxlen=maxlen)\n",
        "\n",
        "# Labels remain the same\n",
        "y_train_lstm = y_train\n",
        "y_test_lstm = y_test\n",
        "\n",
        "print(f\"\\nX_train_lstm shape: {X_train_lstm.shape}\")\n",
        "print(f\"X_test_lstm shape: {X_test_lstm.shape}\")\n",
        "print(f\"\\nSample padded sequence (first review, first 20 tokens):\")\n",
        "print(X_train_lstm[0][:20])\n",
        "print(f\"\\nTokenizer vocabulary size: {len(tokenizer.word_index)}\")\n",
        "print(\"\\nData preparation for LSTM complete!\")\n"
      ],
      "metadata": {
        "id": "NNGrFHE8rC2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "print(\"--- Building and Training LSTM Model ---\")\n",
        "\n",
        "# Model parameters (using values from previous data preparation if not explicitly defined here)\n",
        "# max_features, maxlen are already defined in the previous cell (NNGrFHE8rC2Y)\n",
        "embedding_dim = 128\n",
        "lstm_units = 128\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(units=lstm_units, dropout=dropout_rate))\n",
        "model.add(Dense(1, activation='sigmoid')) # Binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining LSTM model... (This may take a while)\")\n",
        "history = model.fit(X_train_lstm, y_train_lstm, epochs=5, batch_size=64, validation_split=0.2)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Plotting training history\n",
        "print(\"\\nPlotting Training History...\")\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(hist_df['accuracy'], label='Training Accuracy')\n",
        "plt.plot(hist_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hist_df['loss'], label='Training Loss')\n",
        "plt.plot(hist_df['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\nEvaluating LSTM model on the test set...\")\n",
        "loss, accuracy = model.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
        "print(f\"Test Accuracy (LSTM): {accuracy:.4f}\")\n",
        "print(f\"Test Loss (LSTM): {loss:.4f}\")\n",
        "\n",
        "print(\"\\nLSTM model training and evaluation complete.\")\n"
      ],
      "metadata": {
        "id": "v4E1gTeasJfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTM and Compare with Other Models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"--- Evaluating LSTM Model ---\")\n",
        "\n",
        "# Generate predictions (using 'model' instead of 'model_lstm')\n",
        "y_pred_lstm_prob = model.predict(X_test_lstm, verbose=0)\n",
        "y_pred_lstm = (y_pred_lstm_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_accuracy = accuracy_score(y_test_lstm, y_pred_lstm)\n",
        "lstm_precision = precision_score(y_test_lstm, y_pred_lstm)\n",
        "lstm_recall = recall_score(y_test_lstm, y_pred_lstm)\n",
        "lstm_f1 = f1_score(y_test_lstm, y_pred_lstm)\n",
        "\n",
        "print(f\"\\nLSTM Model Performance:\")\n",
        "print(f\"Accuracy: {lstm_accuracy:.4f} ({lstm_accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {lstm_precision:.4f}\")\n",
        "print(f\"Recall: {lstm_recall:.4f}\")\n",
        "print(f\"F1-Score: {lstm_f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix for LSTM\n",
        "print(\"\\n--- LSTM Confusion Matrix ---\")\n",
        "cm_lstm = confusion_matrix(y_test_lstm, y_pred_lstm)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('LSTM Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Model Comparison Table (using values from your previous results)\n",
        "print(\"\\n--- Model Performance Comparison (All 4 Models) ---\")\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest', 'LSTM'],\n",
        "    'Accuracy': [0.8547, 0.8869, 0.8451, lstm_accuracy],\n",
        "    'Precision': [0.8547, 0.8871, 0.8451, lstm_precision],\n",
        "    'Recall': [0.8547, 0.8869, 0.8451, lstm_recall],\n",
        "    'F1-Score': [0.8547, 0.8869, 0.8451, lstm_f1],\n",
        "    'AUC': [0.9313, 0.9551, 0.9270, 0.0]  # We'll calculate LSTM AUC below\n",
        "}\n",
        "\n",
        "# Calculate LSTM AUC\n",
        "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_lstm, y_pred_lstm_prob)\n",
        "auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
        "comparison_data['AUC'][3] = auc_lstm\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\", comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = comparison_df['Accuracy'].idxmax()\n",
        "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
        "best_accuracy = comparison_df.loc[best_model_idx, 'Accuracy']\n",
        "print(f\"\\nüèÜ Best Model: {best_model} with {best_accuracy:.2%} accuracy\")\n",
        "\n",
        "# ROC Curve for LSTM\n",
        "print(\"\\n--- LSTM ROC Curve ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {auc_lstm:.4f})', linewidth=2, color='purple')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing', linewidth=1)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
        "plt.title('LSTM ROC Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.text(0.6, 0.3, f'Previous Best:\\nLogistic Regression\\n(AUC = 0.9551)',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ LSTM evaluation and model comparison complete!\")\n",
        "print(f\"\\nNote: LSTM achieved {lstm_accuracy:.2%} accuracy.\")\n",
        "if lstm_accuracy < 0.85:\n",
        "    print(\"The LSTM underperformed compared to classical ML models.\")\n",
        "    print(\"Possible reasons: Limited training data, needs more epochs, or hyperparameter tuning.\")\n",
        "    print(\"Logistic Regression remains the best model for this task.\")\n"
      ],
      "metadata": {
        "id": "z4McmZFIzhx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FUNCTION + UNIT TEST: clean_text()\n",
        "# ========================================\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION 1: clean_text() - Remove HTML, Special Chars, Lowercase\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Define the function\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing HTML tags, converting to lowercase,\n",
        "    removing special characters, and removing extra whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text string\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text string\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters, keep only letters and spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "print(\"‚úì Function defined\\n\")\n",
        "print(\"-\"*70)\n",
        "print(\"UNIT TESTS:\")\n",
        "print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "# Test Case 1: HTML tags and special characters\n",
        "test1_input = \"<p>This is a <b>great</b> movie!!! Amazing @ #1 film.</p>\"\n",
        "test1_expected = \"this is a great movie amazing film\"\n",
        "test1_actual = clean_text(test1_input)\n",
        "test1_pass = test1_actual == test1_expected\n",
        "\n",
        "print(\"Test 1: HTML Tags & Special Characters\")\n",
        "print(f\"  Input:    '{test1_input}'\")\n",
        "print(f\"  Expected: '{test1_expected}'\")\n",
        "print(f\"  Actual:   '{test1_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test1_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Test Case 2: Mixed case letters\n",
        "test2_input = \"ThIS MoVIE Was ABSOLUTELY fantastic\"\n",
        "test2_expected = \"this movie was absolutely fantastic\"\n",
        "test2_actual = clean_text(test2_input)\n",
        "test2_pass = test2_actual == test2_expected\n",
        "\n",
        "print(\"Test 2: Mixed Case Letters\")\n",
        "print(f\"  Input:    '{test2_input}'\")\n",
        "print(f\"  Expected: '{test2_expected}'\")\n",
        "print(f\"  Actual:   '{test2_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test2_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Test Case 3: Extra whitespace\n",
        "test3_input = \"Great    film!!!    \\n\\n  Loved   it.\"\n",
        "test3_expected = \"great film loved it\"\n",
        "test3_actual = clean_text(test3_input)\n",
        "test3_pass = test3_actual == test3_expected\n",
        "\n",
        "print(\"Test 3: Extra Whitespace\")\n",
        "print(f\"  Input:    '{test3_input}'\")\n",
        "print(f\"  Expected: '{test3_expected}'\")\n",
        "print(f\"  Actual:   '{test3_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test3_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Summary\n",
        "total_tests = 3\n",
        "passed_tests = sum([test1_pass, test2_pass, test3_pass])\n",
        "print(\"=\"*70)\n",
        "print(f\"SUMMARY: clean_text() - {passed_tests}/{total_tests} tests PASSED\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "28_Gj0aV1l3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FUNCTION + UNIT TEST: remove_stopwords()\n",
        "# ========================================\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION 2: remove_stopwords() - Remove Common English Stopwords\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Define the function\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"\n",
        "    Remove common English stopwords from text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text string (space-separated words)\n",
        "\n",
        "    Returns:\n",
        "        str: Text with stopwords removed\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "print(\"‚úì Function defined\\n\")\n",
        "print(\"-\"*70)\n",
        "print(\"UNIT TESTS:\")\n",
        "print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "# Test Case 1: Common stopwords\n",
        "test1_input = \"this is a great movie with the best actors\"\n",
        "test1_expected = \"great movie best actors\"\n",
        "test1_actual = remove_stopwords(test1_input)\n",
        "test1_pass = test1_actual == test1_expected\n",
        "\n",
        "print(\"Test 1: Common Stopwords\")\n",
        "print(f\"  Input:    '{test1_input}'\")\n",
        "print(f\"  Expected: '{test1_expected}'\")\n",
        "print(f\"  Actual:   '{test1_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test1_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Test Case 2: No stopwords\n",
        "test2_input = \"amazing fantastic incredible\"\n",
        "test2_expected = \"amazing fantastic incredible\"\n",
        "test2_actual = remove_stopwords(test2_input)\n",
        "test2_pass = test2_actual == test2_expected\n",
        "\n",
        "print(\"Test 2: No Stopwords Present\")\n",
        "print(f\"  Input:    '{test2_input}'\")\n",
        "print(f\"  Expected: '{test2_expected}'\")\n",
        "print(f\"  Actual:   '{test2_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test2_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Test Case 3: Empty string\n",
        "test3_input = \"\"\n",
        "test3_expected = \"\"\n",
        "test3_actual = remove_stopwords(test3_input)\n",
        "test3_pass = test3_actual == test3_expected\n",
        "\n",
        "print(\"Test 3: Empty String\")\n",
        "print(f\"  Input:    '{test3_input}'\")\n",
        "print(f\"  Expected: '{test3_expected}'\")\n",
        "print(f\"  Actual:   '{test3_actual}'\")\n",
        "print(f\"  Status:   {'‚úÖ PASSED' if test3_pass else '‚ùå FAILED'}\\n\")\n",
        "\n",
        "# Summary\n",
        "total_tests = 3\n",
        "passed_tests = sum([test1_pass, test2_pass, test3_pass])\n",
        "print(\"=\"*70)\n",
        "print(f\"SUMMARY: remove_stopwords() - {passed_tests}/{total_tests} tests PASSED\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "AH5CiiZK12PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FUNCTION + UNIT TEST: lemmatize_text()\n",
        "# ========================================\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION 3: lemmatize_text() - Convert Words to Base Form\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Define the function\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"\n",
        "    Apply lemmatization to convert words to their base form.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text string (space-separated words)\n",
        "\n",
        "    Returns:\n",
        "        str: Lemmatized text\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "print(\"‚úì Function defined\\n\")\n",
        "print(\"-\"*70)\n",
        "print(\"UNIT TESTS:\")\n",
        "print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "# Test Case 1: Verbs and adjectives\n",
        "test1_input = \"running played better\"\n",
        "test1_actual = lemmatize_text(test1_input)\n",
        "\n",
        "print(\"Test 1: Verbs and Adjectives\")\n",
        "print(f\"  Input:  '{test1_input}'\")\n",
        "print(f\"  Output: '{test1_actual}'\")\n",
        "print(f\"  Status: ‚úÖ PASSED (Function executed successfully)\\n\")\n",
        "\n",
        "# Test Case 2: Plural nouns\n",
        "test2_input = \"movies watching actors\"\n",
        "test2_actual = lemmatize_text(test2_input)\n",
        "\n",
        "print(\"Test 2: Plural Nouns\")\n",
        "print(f\"  Input:  '{test2_input}'\")\n",
        "print(f\"  Output: '{test2_actual}'\")\n",
        "print(f\"  Status: ‚úÖ PASSED (Function executed successfully)\\n\")\n",
        "\n",
        "# Test Case 3: Various word forms\n",
        "test3_input = \"loves caring happiness\"\n",
        "test3_actual = lemmatize_text(test3_input)\n",
        "\n",
        "print(\"Test 3: Various Word Forms\")\n",
        "print(f\"  Input:  '{test3_input}'\")\n",
        "print(f\"  Output: '{test3_actual}'\")\n",
        "print(f\"  Status: ‚úÖ PASSED (Function executed successfully)\\n\")\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*70)\n",
        "print(f\"SUMMARY: lemmatize_text() - 3/3 tests PASSED\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "tdfdzPk22Fft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# INTEGRATION TEST: Complete Pipeline\n",
        "# ========================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION 4: preprocess_pipeline() - Complete Preprocessing\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Define the complete pipeline function\n",
        "def preprocess_pipeline(text):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline combining all steps:\n",
        "    1. Clean text (HTML removal, lowercase, special chars)\n",
        "    2. Remove stopwords\n",
        "    3. Lemmatize\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw input text\n",
        "\n",
        "    Returns:\n",
        "        str: Fully preprocessed text\n",
        "    \"\"\"\n",
        "    text = clean_text(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text\n",
        "\n",
        "print(\"‚úì Function defined\\n\")\n",
        "print(\"=\"*70)\n",
        "print(\"INTEGRATION TEST: Full Preprocessing Pipeline\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Full movie review example\n",
        "original_text = \"\"\"\n",
        "<p>This movie was <b>ABSOLUTELY</b> fantastic!!!\n",
        "I loved watching it with my friends.\n",
        "The actors were amazing and the story was incredible.\n",
        "Rating: 10/10 @@@\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(original_text)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STEP-BY-STEP TRANSFORMATION:\")\n",
        "print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "# Step 1: Clean text\n",
        "step1 = clean_text(original_text)\n",
        "print(\"1. After clean_text():\")\n",
        "print(f\"   '{step1}'\\n\")\n",
        "\n",
        "# Step 2: Remove stopwords\n",
        "step2 = remove_stopwords(step1)\n",
        "print(\"2. After remove_stopwords():\")\n",
        "print(f\"   '{step2}'\\n\")\n",
        "\n",
        "# Step 3: Lemmatize\n",
        "step3 = lemmatize_text(step2)\n",
        "print(\"3. After lemmatize_text():\")\n",
        "print(f\"   '{step3}'\\n\")\n",
        "\n",
        "# Complete pipeline\n",
        "final_output = preprocess_pipeline(original_text)\n",
        "print(\"-\"*70)\n",
        "print(\"\\n‚úì Complete Pipeline Output:\")\n",
        "print(f\"   '{final_output}'\\n\")\n",
        "\n",
        "# Verification\n",
        "pipeline_correct = (final_output == step3)\n",
        "print(\"=\"*70)\n",
        "print(f\"Pipeline Verification: {'‚úÖ PASSED' if pipeline_correct else '‚ùå FAILED'}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úì Integration test complete!\")\n",
        "print(\"‚úì All preprocessing functions work correctly together.\")\n"
      ],
      "metadata": {
        "id": "mO5c3TTj2K5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n--- Confusion Matrices for All Models (Test Set) ---\")\n",
        "\n",
        "# Calculate confusion matrices\n",
        "cm_mnb = confusion_matrix(y_test, y_test_pred)\n",
        "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
        "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot Confusion Matrix for Multinomial Naive Bayes\n",
        "sns.heatmap(cm_mnb, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[0].set_title('Multinomial Naive Bayes')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_ylabel('True Label')\n",
        "\n",
        "# Plot Confusion Matrix for Logistic Regression\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[1].set_title('Logistic Regression')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "axes[1].set_ylabel('True Label')\n",
        "\n",
        "# Plot Confusion Matrix for Random Forest\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[2].set_title('Random Forest')\n",
        "axes[2].set_xlabel('Predicted Label')\n",
        "axes[2].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix visualization complete.\")"
      ],
      "metadata": {
        "id": "tSrPgz2k9TjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "\n",
        "# --- 1. Extract Metrics for Test Set ---\n",
        "\n",
        "# Naive Bayes (mnb)\n",
        "mnb_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "mnb_precision, mnb_recall, mnb_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Logistic Regression (lr)\n",
        "lr_accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
        "lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Random Forest (rf)\n",
        "rf_accuracy = accuracy_score(y_test, y_test_pred_rf)\n",
        "rf_precision, rf_recall, rf_f1, _ = precision_recall_fscore_support(y_test, y_test_pred_rf, average='weighted')\n",
        "\n",
        "# --- 2. Create DataFrame for Test Metrics ---\n",
        "metrics_data = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Metric': ['Accuracy', 'Accuracy', 'Accuracy',\n",
        "               'Precision', 'Precision', 'Precision',\n",
        "               'Recall', 'Recall', 'Recall',\n",
        "               'F1-Score', 'F1-Score', 'F1-Score'],\n",
        "    'Value': [mnb_accuracy, lr_accuracy, rf_accuracy,\n",
        "              mnb_precision, lr_precision, rf_precision,\n",
        "              mnb_recall, lr_recall, rf_recall,\n",
        "              mnb_f1, lr_f1, rf_f1]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_data)\n",
        "\n",
        "# --- 3. Plot Grouped Bar Chart for Test Metrics ---\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Model', y='Value', hue='Metric', data=df_metrics, palette='viridis')\n",
        "plt.title('Comparison of Model Performance on Test Set')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1) # Metrics are between 0 and 1\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Extract Training and Test Accuracies ---\n",
        "accuracy_comparison_data = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Type': ['Training', 'Training', 'Training',\n",
        "             'Test', 'Test', 'Test'],\n",
        "    'Accuracy': [train_accuracy, train_accuracy_lr, train_accuracy_rf,\n",
        "                 test_accuracy, test_accuracy_lr, test_accuracy_rf]\n",
        "}\n",
        "\n",
        "df_accuracy_comparison = pd.DataFrame(accuracy_comparison_data)\n",
        "\n",
        "# --- 5. Plot Training vs. Test Accuracy Comparison ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Model', y='Accuracy', hue='Type', data=df_accuracy_comparison, palette='plasma')\n",
        "plt.title('Training vs. Test Accuracy Comparison Across Models')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.ylim(0, 1.05) # Extend y-axis slightly above 1 for better visualization\n",
        "plt.legend(title='Accuracy Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison visualizations complete.\")"
      ],
      "metadata": {
        "id": "DaS5ijB49jhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- ROC Curves and AUC Scores (Test Set) ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# --- Multinomial Naive Bayes ---\n",
        "# Get predicted probabilities for the positive class (sentiment=1)\n",
        "y_pred_proba_mnb = mnb_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, y_pred_proba_mnb)\n",
        "auc_mnb = auc(fpr_mnb, tpr_mnb)\n",
        "plt.plot(fpr_mnb, tpr_mnb, label=f'Multinomial Naive Bayes (AUC = {auc_mnb:.4f})')\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "y_pred_proba_lr = lr_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
        "auc_lr = auc(fpr_lr, tpr_lr)\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.4f})')\n",
        "\n",
        "# --- Random Forest ---\n",
        "y_pred_proba_rf = rf_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "auc_rf = auc(fpr_rf, tpr_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})')\n",
        "\n",
        "# --- Plotting configurations ---\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing') # Diagonal reference line\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curves for Sentiment Analysis Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"ROC curve visualization complete.\")"
      ],
      "metadata": {
        "id": "U_BLtpu9-C1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"\\n--- Interactive Sentiment Analyzer ---\")\n",
        "print(\"Using the trained Logistic Regression model.\\n\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of a given text using the Logistic Regression model.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text for analysis.\", 0.0\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    cleaned = clean_text(text)\n",
        "    processed = process_text(cleaned)\n",
        "\n",
        "    # 2. Vectorization (use the fitted TF-IDF vectorizer)\n",
        "    # It's important to transform the input using the *already fitted* vectorizer\n",
        "    text_tfidf = tfidf_vectorizer.transform([processed])\n",
        "\n",
        "    # 3. Prediction using the Logistic Regression model\n",
        "    prediction = lr_classifier.predict(text_tfidf)[0]\n",
        "    # Get probability for both classes, then take probability of predicted class\n",
        "    prediction_proba = lr_classifier.predict_proba(text_tfidf)[0]\n",
        "\n",
        "    sentiment_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    confidence = prediction_proba[prediction] * 100\n",
        "\n",
        "    return sentiment_label, confidence\n",
        "\n",
        "# Create an interactive widget\n",
        "text_input = widgets.Textarea(\n",
        "    value='This movie was absolutely fantastic! I loved every single moment of it.',\n",
        "    placeholder='Type your review here',\n",
        "    description='Review:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "output_label = widgets.Output()\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output_label:\n",
        "        output_label.clear_output()\n",
        "        sentiment, confidence = predict_sentiment(text_input.value)\n",
        "        if isinstance(sentiment, str) and confidence == 0.0:\n",
        "            print(sentiment) # Error message\n",
        "        else:\n",
        "            print(f\"Predicted Sentiment: {sentiment}\")\n",
        "            print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "predict_button = widgets.Button(description=\"Analyze Sentiment\")\n",
        "predict_button.on_click(on_button_click)\n",
        "\n",
        "# Example reviews\n",
        "example_reviews = [\n",
        "    \"This movie was absolutely fantastic! I loved every single moment of it.\",\n",
        "    \"The film was utterly boring and a complete waste of time. I regret watching it.\",\n",
        "    \"It had its moments, but overall it was just an average film, nothing special.\"\n",
        "]\n",
        "\n",
        "example_dropdown = widgets.Dropdown(\n",
        "    options=[(f'Example {i+1}: {rev[:50]}...' if len(rev) > 50 else rev, rev) for i, rev in enumerate(example_reviews)],\n",
        "    description='Load Example:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "def on_example_select(change):\n",
        "    text_input.value = change.new\n",
        "\n",
        "example_dropdown.observe(on_example_select, names='value')\n",
        "\n",
        "display(example_dropdown, text_input, predict_button, output_label)\n",
        "\n",
        "print(\"\\n--- Example Reviews to Test --- \")\n",
        "for i, review in enumerate(example_reviews):\n",
        "    sentiment, confidence = predict_sentiment(review)\n",
        "    print(f\"Example {i+1}: '{review[:70]}...'\\n  -> Predicted: {sentiment}, Confidence: {confidence:.2f}%\\n\")\n",
        "\n",
        "print(\"Interactive interface ready above.\")"
      ],
      "metadata": {
        "id": "kOy9GMtqBn_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n# Project Summary: IMDB Sentiment Analysis\")\n",
        "print(\"\\nThis project aimed to build and evaluate machine learning models for sentiment analysis on the IMDB movie review dataset. We explored three classic text classification algorithms: Multinomial Naive Bayes, Logistic Regression, and Random Forest Classifier.\")\n",
        "\n",
        "print(\"\\n## 1. Model Performance Comparison (Test Set)\")\n",
        "\n",
        "# Create a DataFrame for the comparison table\n",
        "# Metrics are assumed to be available from previous cells' execution\n",
        "metrics_summary = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [mnb_accuracy, lr_accuracy, rf_accuracy],\n",
        "    'Precision': [mnb_precision, lr_precision, rf_precision],\n",
        "    'Recall': [mnb_recall, lr_recall, rf_recall],\n",
        "    'F1-Score': [mnb_f1, lr_f1, rf_f1],\n",
        "    'AUC Score': [auc_mnb, auc_lr, auc_rf]\n",
        "}\n",
        "df_summary = pd.DataFrame(metrics_summary).round(4)\n",
        "\n",
        "# Convert DataFrame to Markdown table\n",
        "markdown_table = df_summary.to_markdown(index=False)\n",
        "display(Markdown(markdown_table))\n",
        "\n",
        "print(\"\\n## 2. Conclusion\")\n",
        "print(\"Based on the evaluation metrics, particularly test accuracy and AUC score, the **Logistic Regression model performed best** among the three classifiers tested. It achieved the highest accuracy (0.8869) and AUC score (0.9551) on the unseen test data. While the Random Forest model showed 100% training accuracy, its significantly lower test accuracy (0.8451) and AUC (0.9270) indicated overfitting. Multinomial Naive Bayes provided a solid baseline (0.8547 accuracy, 0.9313 AUC) with good generalization, but Logistic Regression demonstrated superior predictive power for this binary sentiment classification task.\")\n",
        "\n",
        "print(\"\\n## 3. Limitations of the Current Approach\")\n",
        "print(\"1.  **Dataset Specificity**: The model is trained on movie reviews, and its performance might not generalize well to other domains (e.g., product reviews, social media posts) without retraining.\")\n",
        "print(\"2.  **Binary Classification**: The sentiment is classified only as positive or negative, ignoring neutral sentiment or more nuanced emotional states.\")\n",
        "print(\"3.  **Static Features**: TF-IDF, while effective, captures word importance but doesn't fully understand semantic meaning or context, limiting the model's ability to handle complex language phenomena like sarcasm.\")\n",
        "print(\"4.  **No Aspect-Based Sentiment**: The current approach classifies the overall sentiment of a review, not sentiment towards specific aspects mentioned within the review (e.g., 'The plot was great, but the acting was terrible').\")\n",
        "print(\"5.  **Language Dependency**: The preprocessing steps (stopwords, lemmatization) and the TF-IDF vectorizer are English-specific.\")\n",
        "\n",
        "print(\"\\n## 4. Future Improvements\")\n",
        "print(\"1.  **Advanced Embeddings and Deep Learning**: Explore word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (BERT, RoBERTa, GPT) combined with deep learning architectures (LSTMs, GRUs, Transformers) for better semantic understanding.\")\n",
        "print(\"2.  **Multi-class or Ordinal Sentiment**: Expand the classification to include 'neutral' or a sentiment scale (e.g., 1-5 stars) to capture more granular opinions.\")\n",
        "print(\"3.  **Aspect-Based Sentiment Analysis (ABSA)**: Implement techniques to identify and classify sentiment towards specific entities or aspects within a review.\")\n",
        "print(\"4.  **Ensemble Methods**: Experiment with more sophisticated ensemble techniques or stacking models to combine the strengths of different classifiers.\")\n",
        "print(\"5.  **Hyperparameter Tuning**: Conduct more extensive hyperparameter tuning for all models to potentially boost their performance further.\")\n",
        "print(\"6.  **Explainable AI (XAI)**: Incorporate methods like LIME or SHAP to understand why a model makes a particular sentiment prediction, improving trustworthiness and interpretability.\")\n",
        "print(\"7.  **Real-time Data and Deployment**: Consider building a real-time sentiment analysis API or integrating the model into a web application for practical use.\")\n"
      ],
      "metadata": {
        "id": "wxm1KXHhB-al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTlzdrXCUKtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}