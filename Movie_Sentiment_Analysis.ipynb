{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPQg5udkKexXNNyMq1NwAkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghadiiz/movie-sentiment-analyzer-nlp/blob/main/Movie_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rodl3p0twt-q"
      },
      "outputs": [],
      "source": [
        "def setup_nlp_environment():\n",
        "    \"\"\"Imports common NLP libraries and downloads NLTK data packages.\"\"\"\n",
        "    print(\"Importing necessary libraries...\")\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        import nltk\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.naive_bayes import MultinomialNB\n",
        "        from sklearn.metrics import classification_report, accuracy_score\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            from tensorflow import keras\n",
        "        except ImportError:\n",
        "            print(\"TensorFlow/Keras not found. Skipping import.\")\n",
        "\n",
        "        print(\"Libraries imported successfully.\")\n",
        "\n",
        "        print(\"Downloading NLTK data packages...\")\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        nltk.download('omw-1.4', quiet=True) # Open Multilingual WordNet, often needed with wordnet\n",
        "        nltk.download('punkt_tab', quiet=True) # Added to resolve LookupError\n",
        "        print(\"NLTK data packages downloaded successfully.\")\n",
        "\n",
        "        # Optional: Set up plotting style\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"Error importing a library: {e}. Please ensure all libraries are installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Call the function to set up the environment\n",
        "setup_nlp_environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# 1. Load the IMDB dataset\n",
        "# num_words parameter keeps the most frequent words\n",
        "max_features = 10000  # consider only top 10k words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# 2. Get the word index and create a reverse word index\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# The indices are offset by 3 because 0, 1, and 2 are reserved for \"padding,\" \"start of sequence,\" and \"unknown.\"\n",
        "# Define a decoding function\n",
        "def decode_review(text_sequence):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text_sequence])\n",
        "\n",
        "# 3. Convert integer sequences back to text reviews\n",
        "train_reviews_text = [decode_review(seq) for seq in x_train]\n",
        "test_reviews_text = [decode_review(seq) for seq in x_test]\n",
        "\n",
        "# 4. Create pandas DataFrames\n",
        "# Combine train and test data\n",
        "reviews = train_reviews_text + test_reviews_text\n",
        "sentiments = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "imdb_df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
        "\n",
        "# Map sentiment labels to 'positive' and 'negative' for clarity if desired\n",
        "# imdb_df['sentiment'] = imdb_df['sentiment'].map({0: 'negative', 1: 'positive'})\n",
        "\n",
        "# 5. Display the first few rows, DataFrame shape, and data types\n",
        "print(\"\\n--- IMDB Dataset DataFrame ---\")\n",
        "print(\"First 5 rows:\")\n",
        "print(imdb_df.head())\n",
        "\n",
        "print(f\"\\nDataFrame shape: {imdb_df.shape}\")\n",
        "\n",
        "print(\"\\nDataFrame info:\")\n",
        "imdb_df.info()"
      ],
      "metadata": {
        "id": "rpl0sxSR1sqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n--- Exploratory Data Analysis on IMDB Dataset ---\")\n",
        "\n",
        "# 1) Bar chart showing count of positive vs negative reviews\n",
        "print(\"\\n1. Sentiment Distribution:\")\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(x='sentiment', data=imdb_df)\n",
        "plt.title('Distribution of Sentiments (0: Negative, 1: Positive)')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'])\n",
        "plt.show()\n",
        "\n",
        "# 2) Histogram showing distribution of review lengths in words\n",
        "print(\"\\n2. Review Length Distribution:\")\n",
        "# Calculate review lengths\n",
        "imdb_df['review_length'] = imdb_df['review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(imdb_df['review_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Review Lengths (in words)')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 3) Check for missing values\n",
        "print(\"\\n3. Missing Values Check:\")\n",
        "print(imdb_df.isnull().sum())\n",
        "\n",
        "# 4) Display 5 random sample reviews with their sentiments\n",
        "print(\"\\n4. Five Random Sample Reviews:\")\n",
        "for index, row in imdb_df.sample(5).iterrows():\n",
        "    print(f\"\\nSentiment: {'Positive' if row['sentiment'] == 1 else 'Negative'}\")\n",
        "    print(f\"Review: {row['review']}\")\n",
        "\n",
        "# Drop the temporary 'review_length' column if no longer needed\n",
        "imdb_df = imdb_df.drop(columns=['review_length'])"
      ],
      "metadata": {
        "id": "P0ii4zGw2PkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# from bs4 import BeautifulSoup # Uncomment if you prefer BeautifulSoup for HTML removal and install it\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Performs text preprocessing steps:\n",
        "    1. Converts to lowercase.\n",
        "    2. Removes HTML tags.\n",
        "    3. Removes special characters and punctuation (keeping only letters and spaces).\n",
        "    4. Removes extra whitespaces.\n",
        "    \"\"\"\n",
        "    # 1. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove HTML tags (using regex for simplicity as BeautifulSoup might require installation)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # If using BeautifulSoup: text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    # 3. Remove special characters and punctuation (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # 4. Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to create a new 'cleaned_review' column\n",
        "print(\"Applying text cleaning to 'review' column...\")\n",
        "imdb_df['cleaned_review'] = imdb_df['review'].apply(clean_text)\n",
        "print(\"Cleaning complete. 'cleaned_review' column created.\")\n",
        "\n",
        "# Display before and after examples of 3 random reviews\n",
        "print(\"\\n--- Before and After Cleaning Examples (3 Reviews) ---\")\n",
        "sample_reviews = imdb_df.sample(3)\n",
        "\n",
        "for index, row in sample_reviews.iterrows():\n",
        "    print(f\"\\nOriginal Review:\\n{row['review']}\")\n",
        "    print(f\"Cleaned Review:\\n{row['cleaned_review']}\")\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "7wSuHH7y3r1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Re-import for explicit use if not already in scope, though it is.\n",
        "\n",
        "# Initialize NLTK components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text, removes stopwords, and applies lemmatization.\n",
        "    \"\"\"\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            processed_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "print(\"Applying tokenization, stopword removal, and lemmatization to 'cleaned_review' column...\")\n",
        "imdb_df['processed_review'] = imdb_df['cleaned_review'].apply(process_text)\n",
        "print(\"Processing complete. 'processed_review' column created.\")\n",
        "\n",
        "# Display before and after examples of 3 random reviews\n",
        "print(\"\\n--- Before and After Processing Examples (3 Reviews) ---\")\n",
        "sample_reviews_processed = imdb_df.sample(3)\n",
        "\n",
        "for index, row in sample_reviews_processed.iterrows():\n",
        "    print(f\"\\nOriginal Review (Cleaned):\\n{row['cleaned_review']}\")\n",
        "    print(f\"Processed Review:\\n{row['processed_review']}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "# Display 20 most common words after processing\n",
        "print(\"\\n--- 20 Most Common Words After Processing ---\")\n",
        "all_words = ' '.join(imdb_df['processed_review']).split()\n",
        "word_counts = Counter(all_words)\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Count', y='Word', data=common_words_df)\n",
        "plt.title('20 Most Common Words in Processed Reviews')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Word')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oe1aX2Br4BA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n--- Data Splitting and TF-IDF Vectorization ---\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = imdb_df['processed_review']\n",
        "y = imdb_df['sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets (80/20 split) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Dataset split into training and testing sets.\")\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "# max_features is set to 5000 as requested, considering the top 5000 most frequent words\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit the vectorizer on the training data only\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Text data vectorized using TF-IDF.\")\n",
        "\n",
        "# Display shapes of the resulting datasets\n",
        "print(\"\\nShapes of the datasets:\")\n",
        "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
        "print(f\"X_test_tfidf shape: {X_test_tfidf.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Display vocabulary size\n",
        "vocabulary_size = len(tfidf_vectorizer.get_feature_names_out())\n",
        "print(f\"\\nVocabulary size (number of features): {vocabulary_size}\")\n",
        "\n",
        "# Display a sample of the TF-IDF matrix\n",
        "# Since TF-IDF matrix is sparse, we convert a small part to dense for display\n",
        "print(\"\\nSample of TF-IDF matrix (first 5 rows and first 10 columns):\")\n",
        "print(X_train_tfidf[:5, :10].toarray())\n",
        "\n",
        "print(\"\\nData preparation complete.\")"
      ],
      "metadata": {
        "id": "ECgBbaLI5Axp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Multinomial Naive Bayes Model Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "mnb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Multinomial Naive Bayes classifier...\")\n",
        "mnb_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = mnb_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = mnb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "print(\"\\nModel evaluation complete.\")"
      ],
      "metadata": {
        "id": "_NIAdAP66NUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Logistic Regression Model Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "# max_iter is increased to ensure convergence for the given dataset\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Logistic Regression classifier...\")\n",
        "lr_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred_lr = lr_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred_lr = lr_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy_lr = accuracy_score(y_train, y_train_pred_lr)\n",
        "print(f\"\\nTraining Accuracy (Logistic Regression): {train_accuracy_lr:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
        "print(f\"Test Accuracy (Logistic Regression): {test_accuracy_lr:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set (Logistic Regression):\")\n",
        "print(classification_report(y_test, y_test_pred_lr))\n",
        "\n",
        "print(\"\\nLogistic Regression Model evaluation complete.\")"
      ],
      "metadata": {
        "id": "mV6kCBpm6jsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n--- Random Forest Classifier Training and Evaluation ---\")\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "# n_estimators is the number of trees in the forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "# Train the classifier on the training data\n",
        "print(\"Training Random Forest classifier... (This may take a few minutes)\")\n",
        "rf_classifier.fit(X_train_tfidf, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred_rf = rf_classifier.predict(X_train_tfidf)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate and display accuracy for training set\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "print(f\"\\nTraining Accuracy (Random Forest): {train_accuracy_rf:.4f}\")\n",
        "\n",
        "# Calculate and display accuracy for test set\n",
        "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
        "print(f\"Test Accuracy (Random Forest): {test_accuracy_rf:.4f}\")\n",
        "\n",
        "# Display classification report for the test set\n",
        "print(\"\\nClassification Report for Test Set (Random Forest):\")\n",
        "print(classification_report(y_test, y_test_pred_rf))\n",
        "\n",
        "print(\"\\nRandom Forest Classifier evaluation complete.\")"
      ],
      "metadata": {
        "id": "drqkME4077Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n--- Confusion Matrices for All Models (Test Set) ---\")\n",
        "\n",
        "# Calculate confusion matrices\n",
        "cm_mnb = confusion_matrix(y_test, y_test_pred)\n",
        "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
        "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot Confusion Matrix for Multinomial Naive Bayes\n",
        "sns.heatmap(cm_mnb, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[0].set_title('Multinomial Naive Bayes')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_ylabel('True Label')\n",
        "\n",
        "# Plot Confusion Matrix for Logistic Regression\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[1].set_title('Logistic Regression')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "axes[1].set_ylabel('True Label')\n",
        "\n",
        "# Plot Confusion Matrix for Random Forest\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "axes[2].set_title('Random Forest')\n",
        "axes[2].set_xlabel('Predicted Label')\n",
        "axes[2].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix visualization complete.\")"
      ],
      "metadata": {
        "id": "tSrPgz2k9TjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "\n",
        "# --- 1. Extract Metrics for Test Set ---\n",
        "\n",
        "# Naive Bayes (mnb)\n",
        "mnb_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "mnb_precision, mnb_recall, mnb_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "# Logistic Regression (lr)\n",
        "lr_accuracy = accuracy_score(y_test, y_test_pred_lr)\n",
        "lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(y_test, y_test_pred_lr, average='weighted')\n",
        "\n",
        "# Random Forest (rf)\n",
        "rf_accuracy = accuracy_score(y_test, y_test_pred_rf)\n",
        "rf_precision, rf_recall, rf_f1, _ = precision_recall_fscore_support(y_test, y_test_pred_rf, average='weighted')\n",
        "\n",
        "# --- 2. Create DataFrame for Test Metrics ---\n",
        "metrics_data = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Metric': ['Accuracy', 'Accuracy', 'Accuracy',\n",
        "               'Precision', 'Precision', 'Precision',\n",
        "               'Recall', 'Recall', 'Recall',\n",
        "               'F1-Score', 'F1-Score', 'F1-Score'],\n",
        "    'Value': [mnb_accuracy, lr_accuracy, rf_accuracy,\n",
        "              mnb_precision, lr_precision, rf_precision,\n",
        "              mnb_recall, lr_recall, rf_recall,\n",
        "              mnb_f1, lr_f1, rf_f1]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_data)\n",
        "\n",
        "# --- 3. Plot Grouped Bar Chart for Test Metrics ---\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Model', y='Value', hue='Metric', data=df_metrics, palette='viridis')\n",
        "plt.title('Comparison of Model Performance on Test Set')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1) # Metrics are between 0 and 1\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Extract Training and Test Accuracies ---\n",
        "accuracy_comparison_data = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest',\n",
        "              'Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Type': ['Training', 'Training', 'Training',\n",
        "             'Test', 'Test', 'Test'],\n",
        "    'Accuracy': [train_accuracy, train_accuracy_lr, train_accuracy_rf,\n",
        "                 test_accuracy, test_accuracy_lr, test_accuracy_rf]\n",
        "}\n",
        "\n",
        "df_accuracy_comparison = pd.DataFrame(accuracy_comparison_data)\n",
        "\n",
        "# --- 5. Plot Training vs. Test Accuracy Comparison ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Model', y='Accuracy', hue='Type', data=df_accuracy_comparison, palette='plasma')\n",
        "plt.title('Training vs. Test Accuracy Comparison Across Models')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.ylim(0, 1.05) # Extend y-axis slightly above 1 for better visualization\n",
        "plt.legend(title='Accuracy Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Comparison visualizations complete.\")"
      ],
      "metadata": {
        "id": "DaS5ijB49jhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- ROC Curves and AUC Scores (Test Set) ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# --- Multinomial Naive Bayes ---\n",
        "# Get predicted probabilities for the positive class (sentiment=1)\n",
        "y_pred_proba_mnb = mnb_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_mnb, tpr_mnb, _ = roc_curve(y_test, y_pred_proba_mnb)\n",
        "auc_mnb = auc(fpr_mnb, tpr_mnb)\n",
        "plt.plot(fpr_mnb, tpr_mnb, label=f'Multinomial Naive Bayes (AUC = {auc_mnb:.4f})')\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "y_pred_proba_lr = lr_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
        "auc_lr = auc(fpr_lr, tpr_lr)\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.4f})')\n",
        "\n",
        "# --- Random Forest ---\n",
        "y_pred_proba_rf = rf_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "auc_rf = auc(fpr_rf, tpr_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})')\n",
        "\n",
        "# --- Plotting configurations ---\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing') # Diagonal reference line\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curves for Sentiment Analysis Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"ROC curve visualization complete.\")"
      ],
      "metadata": {
        "id": "U_BLtpu9-C1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"\\n--- Interactive Sentiment Analyzer ---\")\n",
        "print(\"Using the trained Logistic Regression model.\\n\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of a given text using the Logistic Regression model.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text for analysis.\", 0.0\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    cleaned = clean_text(text)\n",
        "    processed = process_text(cleaned)\n",
        "\n",
        "    # 2. Vectorization (use the fitted TF-IDF vectorizer)\n",
        "    # It's important to transform the input using the *already fitted* vectorizer\n",
        "    text_tfidf = tfidf_vectorizer.transform([processed])\n",
        "\n",
        "    # 3. Prediction using the Logistic Regression model\n",
        "    prediction = lr_classifier.predict(text_tfidf)[0]\n",
        "    # Get probability for both classes, then take probability of predicted class\n",
        "    prediction_proba = lr_classifier.predict_proba(text_tfidf)[0]\n",
        "\n",
        "    sentiment_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    confidence = prediction_proba[prediction] * 100\n",
        "\n",
        "    return sentiment_label, confidence\n",
        "\n",
        "# Create an interactive widget\n",
        "text_input = widgets.Textarea(\n",
        "    value='This movie was absolutely fantastic! I loved every single moment of it.',\n",
        "    placeholder='Type your review here',\n",
        "    description='Review:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "output_label = widgets.Output()\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output_label:\n",
        "        output_label.clear_output()\n",
        "        sentiment, confidence = predict_sentiment(text_input.value)\n",
        "        if isinstance(sentiment, str) and confidence == 0.0:\n",
        "            print(sentiment) # Error message\n",
        "        else:\n",
        "            print(f\"Predicted Sentiment: {sentiment}\")\n",
        "            print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "predict_button = widgets.Button(description=\"Analyze Sentiment\")\n",
        "predict_button.on_click(on_button_click)\n",
        "\n",
        "# Example reviews\n",
        "example_reviews = [\n",
        "    \"This movie was absolutely fantastic! I loved every single moment of it.\",\n",
        "    \"The film was utterly boring and a complete waste of time. I regret watching it.\",\n",
        "    \"It had its moments, but overall it was just an average film, nothing special.\"\n",
        "]\n",
        "\n",
        "example_dropdown = widgets.Dropdown(\n",
        "    options=[(f'Example {i+1}: {rev[:50]}...' if len(rev) > 50 else rev, rev) for i, rev in enumerate(example_reviews)],\n",
        "    description='Load Example:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "def on_example_select(change):\n",
        "    text_input.value = change.new\n",
        "\n",
        "example_dropdown.observe(on_example_select, names='value')\n",
        "\n",
        "display(example_dropdown, text_input, predict_button, output_label)\n",
        "\n",
        "print(\"\\n--- Example Reviews to Test --- \")\n",
        "for i, review in enumerate(example_reviews):\n",
        "    sentiment, confidence = predict_sentiment(review)\n",
        "    print(f\"Example {i+1}: '{review[:70]}...'\\n  -> Predicted: {sentiment}, Confidence: {confidence:.2f}%\\n\")\n",
        "\n",
        "print(\"Interactive interface ready above.\")"
      ],
      "metadata": {
        "id": "kOy9GMtqBn_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n# Project Summary: IMDB Sentiment Analysis\")\n",
        "print(\"\\nThis project aimed to build and evaluate machine learning models for sentiment analysis on the IMDB movie review dataset. We explored three classic text classification algorithms: Multinomial Naive Bayes, Logistic Regression, and Random Forest Classifier.\")\n",
        "\n",
        "print(\"\\n## 1. Model Performance Comparison (Test Set)\")\n",
        "\n",
        "# Create a DataFrame for the comparison table\n",
        "# Metrics are assumed to be available from previous cells' execution\n",
        "metrics_summary = {\n",
        "    'Model': ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [mnb_accuracy, lr_accuracy, rf_accuracy],\n",
        "    'Precision': [mnb_precision, lr_precision, rf_precision],\n",
        "    'Recall': [mnb_recall, lr_recall, rf_recall],\n",
        "    'F1-Score': [mnb_f1, lr_f1, rf_f1],\n",
        "    'AUC Score': [auc_mnb, auc_lr, auc_rf]\n",
        "}\n",
        "df_summary = pd.DataFrame(metrics_summary).round(4)\n",
        "\n",
        "# Convert DataFrame to Markdown table\n",
        "markdown_table = df_summary.to_markdown(index=False)\n",
        "display(Markdown(markdown_table))\n",
        "\n",
        "print(\"\\n## 2. Conclusion\")\n",
        "print(\"Based on the evaluation metrics, particularly test accuracy and AUC score, the **Logistic Regression model performed best** among the three classifiers tested. It achieved the highest accuracy (0.8869) and AUC score (0.9551) on the unseen test data. While the Random Forest model showed 100% training accuracy, its significantly lower test accuracy (0.8451) and AUC (0.9270) indicated overfitting. Multinomial Naive Bayes provided a solid baseline (0.8547 accuracy, 0.9313 AUC) with good generalization, but Logistic Regression demonstrated superior predictive power for this binary sentiment classification task.\")\n",
        "\n",
        "print(\"\\n## 3. Limitations of the Current Approach\")\n",
        "print(\"1.  **Dataset Specificity**: The model is trained on movie reviews, and its performance might not generalize well to other domains (e.g., product reviews, social media posts) without retraining.\")\n",
        "print(\"2.  **Binary Classification**: The sentiment is classified only as positive or negative, ignoring neutral sentiment or more nuanced emotional states.\")\n",
        "print(\"3.  **Static Features**: TF-IDF, while effective, captures word importance but doesn't fully understand semantic meaning or context, limiting the model's ability to handle complex language phenomena like sarcasm.\")\n",
        "print(\"4.  **No Aspect-Based Sentiment**: The current approach classifies the overall sentiment of a review, not sentiment towards specific aspects mentioned within the review (e.g., 'The plot was great, but the acting was terrible').\")\n",
        "print(\"5.  **Language Dependency**: The preprocessing steps (stopwords, lemmatization) and the TF-IDF vectorizer are English-specific.\")\n",
        "\n",
        "print(\"\\n## 4. Future Improvements\")\n",
        "print(\"1.  **Advanced Embeddings and Deep Learning**: Explore word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (BERT, RoBERTa, GPT) combined with deep learning architectures (LSTMs, GRUs, Transformers) for better semantic understanding.\")\n",
        "print(\"2.  **Multi-class or Ordinal Sentiment**: Expand the classification to include 'neutral' or a sentiment scale (e.g., 1-5 stars) to capture more granular opinions.\")\n",
        "print(\"3.  **Aspect-Based Sentiment Analysis (ABSA)**: Implement techniques to identify and classify sentiment towards specific entities or aspects within a review.\")\n",
        "print(\"4.  **Ensemble Methods**: Experiment with more sophisticated ensemble techniques or stacking models to combine the strengths of different classifiers.\")\n",
        "print(\"5.  **Hyperparameter Tuning**: Conduct more extensive hyperparameter tuning for all models to potentially boost their performance further.\")\n",
        "print(\"6.  **Explainable AI (XAI)**: Incorporate methods like LIME or SHAP to understand why a model makes a particular sentiment prediction, improving trustworthiness and interpretability.\")\n",
        "print(\"7.  **Real-time Data and Deployment**: Consider building a real-time sentiment analysis API or integrating the model into a web application for practical use.\")\n"
      ],
      "metadata": {
        "id": "wxm1KXHhB-al"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}